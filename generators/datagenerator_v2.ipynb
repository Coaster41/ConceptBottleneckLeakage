{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def multi_label_acc(y_hat, y):\n",
    "    return torch.count_nonzero(torch.round(y_hat) == y).item() / y.numel()\n",
    "\n",
    "def classification_acc(y_hat, y):\n",
    "    return torch.count_nonzero(torch.argmax(y_hat, dim=1) == y).item() / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 64\n",
    "num_c = 16\n",
    "num_y = 10\n",
    "k = 0.875 # ratio of x's to use to calculate concepts\n",
    "num_xc = int(k * num_x)\n",
    "num_samples = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinarySigmoid, self).__init__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BinarySigmoid()'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        return x + torch.round(x).detach() - x.detach()\n",
    "    \n",
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.normal_(m.bias)\n",
    "            m.requires_grad_(False)\n",
    "\n",
    "x_to_c = nn.Sequential(\n",
    "    nn.Linear(num_xc, 32),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(32, num_c)\n",
    ")\n",
    "\n",
    "xc_to_y = nn.Sequential(\n",
    "    nn.Linear(num_x-num_xc+num_c, 32),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(32, num_y)\n",
    ")\n",
    "\n",
    "xc_activation = BinarySigmoid()\n",
    "cy_activation = nn.Softmax(dim=1)\n",
    "\n",
    "init_weights(x_to_c)\n",
    "init_weights(xc_to_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 8824,  8721, 10150,  9140, 12090, 10491, 12169,  9148, 10005,  9262]))\n"
     ]
    }
   ],
   "source": [
    "def gen_x(num_x, batch_size):\n",
    "    # return torch.rand((batch_size, num_x))\n",
    "    return torch.normal(0.5,0.63,(batch_size,num_x))\n",
    "    data = []\n",
    "    num_gaussians = 1\n",
    "    for i in range(num_gaussians):\n",
    "        mean = np.random.rand(num_x)\n",
    "        A = np.random.rand(num_x, num_x)\n",
    "        covariance = np.dot(A, A.T)\n",
    "        data.append(torch.tensor(np.random.multivariate_normal(mean, covariance, batch_size//num_gaussians)).float())\n",
    "    return torch.cat(data, dim=0)\n",
    "\n",
    "x = gen_x(num_x, num_samples)\n",
    "c_logits = x_to_c(x[:,:num_xc])\n",
    "mean_c_logits = torch.mean(c_logits, dim=0)\n",
    "x_to_c[-1].bias -= mean_c_logits\n",
    "c_logits = x_to_c(x[:,:num_xc])\n",
    "c = xc_activation(c_logits)\n",
    "# y_logits = xc_to_y(torch.concat((x[:,num_xc:],c), dim=1))\n",
    "# mean_y_logits = torch.mean(y_logits, dim=0)\n",
    "# xc_to_y[-1].bias -= mean_y_logits\n",
    "# y_logits = xc_to_y(torch.concat((x[:,num_xc:],c), dim=1))\n",
    "# # y = cy_activation(y_logits)\n",
    "# y = cy_activation(y_logits / torch.std(y_logits, dim=0))\n",
    "# y_argmax = torch.argmax(y, dim=1)\n",
    "# pca = PCA(n_components=((num_x-num_xc)//4))\n",
    "# pca.fit(x[:,num_xc:])\n",
    "# x_pca = torch.tensor(pca.transform(x[:,num_xc:])).float()\n",
    "# print(x_pca.shape, torch.concat((x_pca,c), dim=1).shape)\n",
    "kmeans = KMeans(n_clusters=10).fit(torch.concat((x[:,num_xc:],c), dim=1))\n",
    "y_argmax = torch.tensor(kmeans.labels_).long()\n",
    "print(torch.unique(y_argmax, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_to_new_x = nn.Sequential(\n",
    "    nn.Linear(num_x, num_x)\n",
    ")\n",
    "init_weights(x_to_new_x)\n",
    "new_x = x_to_new_x(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_to_c_test = nn.Sequential(\n",
    "    nn.Linear(num_x, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_c),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "xc_to_y_test = nn.Sequential(\n",
    "    nn.Linear(num_c, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_y),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "x_to_y_test = nn.Sequential(\n",
    "    nn.Linear(num_x, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_y),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "xsmall_to_y_test = nn.Sequential(\n",
    "    nn.Linear(num_x-num_xc, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_y),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "xandc_to_y_test = nn.Sequential(\n",
    "    nn.Linear(num_x-num_xc+num_c, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_y),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 10**4\n",
    "y_criterion = nn.CrossEntropyLoss()\n",
    "c_criterion = nn.BCELoss()\n",
    "xc_optimizer = torch.optim.Adam(x_to_c_test.parameters(), lr=0.001)\n",
    "loss_meter = AverageMeter()\n",
    "acc_meter = AverageMeter()\n",
    "cy_optimizer = torch.optim.Adam(xc_to_y_test.parameters(), lr=0.001)\n",
    "xy_optimizer = torch.optim.Adam(x_to_y_test.parameters(), lr=0.001)\n",
    "xsmall_optimizer = torch.optim.Adam(xsmall_to_y_test.parameters(), lr=0.001)\n",
    "xandc_optimizer = torch.optim.Adam(xandc_to_y_test.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.833974838256836 y_acc: 0.625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        # X = y[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        y_pred = x_to_y_test(X)\n",
    "        y_loss = y_criterion(y_pred, Y)\n",
    "        \n",
    "        xy_optimizer.zero_grad()\n",
    "        y_loss.backward()\n",
    "        xy_optimizer.step()\n",
    "        loss_meter.update(y_loss.item(), X.shape[0])\n",
    "        acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "# X = y[index*batch_size:(index+1)*batch_size].to(device)\n",
    "y_pred = x_to_y_test(X)\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08050642907619476 c_acc: 0.84645625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        c_pred = x_to_c_test(X)\n",
    "        c_loss = c_criterion(c_pred, C)\n",
    "        \n",
    "        xc_optimizer.zero_grad()\n",
    "        c_loss.backward()\n",
    "        xc_optimizer.step()\n",
    "        loss_meter.update(c_loss.item(), X.shape[0])\n",
    "        acc_meter.update(multi_label_acc(c_pred, C), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "c_pred = x_to_c_test(X)\n",
    "c_loss = c_criterion(c_pred, C)\n",
    "\n",
    "loss_meter.update(c_loss.item(), X.shape[0])\n",
    "acc_meter.update(multi_label_acc(c_pred, C), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} c_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8681457042694092 y_acc: 0.5933\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        c_pred = torch.round(x_to_c_test(X))\n",
    "        y_pred = xc_to_y_test(c_pred)\n",
    "        y_loss = y_criterion(y_pred, Y)\n",
    "        \n",
    "        cy_optimizer.zero_grad()\n",
    "        y_loss.backward()\n",
    "        cy_optimizer.step()\n",
    "        loss_meter.update(y_loss.item(), X.shape[0])\n",
    "        acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "c_pred = torch.round(x_to_c_test(X))\n",
    "y_pred = xc_to_y_test(c_pred)\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8239235877990723 y_acc: 0.6402\n"
     ]
    }
   ],
   "source": [
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "c_pred = torch.round(x_to_c_test(X))\n",
    "y_pred = xc_to_y_test(C)\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8190937042236328 y_acc: 0.6409\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        y_pred = xc_to_y_test(C)\n",
    "        y_loss = y_criterion(y_pred, Y)\n",
    "        \n",
    "        cy_optimizer.zero_grad()\n",
    "        y_loss.backward()\n",
    "        cy_optimizer.step()\n",
    "        loss_meter.update(y_loss.item(), X.shape[0])\n",
    "        acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "y_pred = xc_to_y_test(C)\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.166325569152832 y_acc: 0.2842\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        # X = y[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        y_pred = xsmall_to_y_test(X[:,num_xc:])\n",
    "        y_loss = y_criterion(y_pred, Y)\n",
    "        \n",
    "        xsmall_optimizer.zero_grad()\n",
    "        y_loss.backward()\n",
    "        xsmall_optimizer.step()\n",
    "        loss_meter.update(y_loss.item(), X.shape[0])\n",
    "        acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "# X = y[index*batch_size:(index+1)*batch_size].to(device)\n",
    "y_pred = xsmall_to_y_test(X[:,num_xc:])\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4932905435562134 y_acc: 0.9754\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_meter.reset()\n",
    "    acc_meter.reset()\n",
    "    for index in range(num_samples//batch_size-1):\n",
    "        X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "        y_pred = xandc_to_y_test(torch.cat((X[:,num_xc:], C), dim=1))\n",
    "        y_loss = y_criterion(y_pred, Y)\n",
    "        \n",
    "        xandc_optimizer.zero_grad()\n",
    "        y_loss.backward()\n",
    "        xandc_optimizer.step()\n",
    "        loss_meter.update(y_loss.item(), X.shape[0])\n",
    "        acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "    # print(f\"Epoch: {epoch} Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")\n",
    "\n",
    "loss_meter.reset()\n",
    "acc_meter.reset()\n",
    "index = num_samples//batch_size-1\n",
    "X, C, Y = x[index*batch_size:(index+1)*batch_size].to(device), c[index*batch_size:(index+1)*batch_size].to(device), y_argmax[index*batch_size:(index+1)*batch_size].to(device)\n",
    "y_pred = xandc_to_y_test(torch.cat((X[:,num_xc:], C), dim=1))\n",
    "y_loss = y_criterion(y_pred, Y)\n",
    "\n",
    "loss_meter.update(y_loss.item(), X.shape[0])\n",
    "acc_meter.update(classification_acc(y_pred, Y), X.shape[0])\n",
    "print(f\"Test Loss: {loss_meter.avg} y_acc: {acc_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
